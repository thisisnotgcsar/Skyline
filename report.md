<h1 style="text-align: center;">High Performance Computing Paper</h1>
<p style="text-align: right; font-style: italic; font-size:150%; margin: auto">
<i>Giulio Golinelli</i>
<br>
<i>0000883007</i>
<br>
<i>23/02/2021</i>
</p>

# Table of Contents <!-- omit in toc -->
- [1. Introduction](#1-introduction)
- [2. Collected Data](#2-collected-data)
- [3. Open-MP Version](#3-open-mp-version)
  - [3.1. Parallelization](#31-parallelization)
  - [3.2. Analysis of Collected Data](#32-analysis-of-collected-data)
- [4. MPI Version](#4-mpi-version)
  - [4.1. Parallelization](#41-parallelization)
  - [4.2. Analysis of Collected Data](#42-analysis-of-collected-data)
- [5. Conclusion](#5-conclusion)
  - [5.1. Strong Scalability](#51-strong-scalability)

# 1. Introduction  
The work carried out consists of parallelizing the **"Skyline" algorithm** using two technologies: **Open-MP** and **MPI**.  
The serial version of the program was provided and was heavily used as the base upon which to develop the other two versions.  
A bash script called **"script.sh"** was also developed to automate the compilation, execution, and verification of the various source files.  
Several tests were performed on the server *isi-raptor03.csr.unibo.it*, their respective execution times were collected, and results based on various characteristics are presented below.

**N.B.**: It is assumed that the reader has full knowledge of:
 - Objective and structure of the Skyline algorithm
 - Architecture and operation of multiprocessor machines, both shared memory and distributed memory
 - C language and programming fundamentals
 - Parallel programming patterns 
 - Performance analysis of parallel programs
 - Open-MP and MPI technologies

**N.B.**: The parallelized versions of the algorithms were kept as similar as possible to the provided serial version to facilitate understanding and to use the latter as a reference point in their explanations.

# 2. Collected Data
All represented timings were collected, for all technologies, taking care to include the smallest fraction of non-parallelizable work.

![Confronto tra le varie tecnologie su test6](./assets/test6.png)
*Image 1: Comparison between the various technologies on the "test6" datafile*

![Rapporto tra speedup e core su test6](./assets/speedup.png)
*Image 2: Ratio between speedup and cores on the "test6" datafile*

# 3. Open-MP Version
## 3.1. Parallelization
The **superlinear speedup** in the Open-MP version is likely due to the fact that at every iteration of the outer `for` loop where the `if` succeeds, a new Open-MP block is opened, and this creates significant additional synchronization and communication overhead which becomes useless with only one processor.
The inner `for` loop modifies elements of the vector that will then be the subjects of subsequent iterations of the outermost `for` loop.  
The outermost `for` loop follows a **critical path**, as the iterations of the inner `for` must necessarily occur before its next iteration.  
It is therefore only possible to parallelize the **innermost `for`** loop, which nonetheless constitutes the majority of the computational cost of the entire algorithm.  
The default scheduling policies were not modified.  
A **reduction operation** was inserted to synchronize the number of remaining points in the skyline each time.
## 3.2. Analysis of Collected Data
The **superlinear speedup** observed is due to the significant additional computational overhead created whenever the outer `for` loop finds a possible candidate for the skyline.  
At every occurrence of this circumstance, an Open-MP section is created and terminated to parallelize the inner loop, which increases computational costs.  
These costs also become useless if the technology is used with only one core.  
If the speedup were calculated using the provided serial version (without the unnecessary Open-MP overhead), we would observe a **linear trend** where superlinear speedup is currently present.

# 4. MPI Version
## 4.1. Parallelization
In the distributed memory system, it was chosen to divide the operating space of the outer `for` loop depending on the rank.  
Let: 
 - `N`: The number of input points
 - `size`: The number of cores used  

Each core works on a portion of $N / size$ input elements, appropriately divided, executes the normal algorithm and produces $N$ output elements.  
Based on the assumption of a correct division of the input (2 cores do not have the possibility of performing the same computation) and relative to each input portion and its respective core, the execution assumes a "serial" behavior.  
The outer `for` still constitutes a critical path, but this is absolutely normal in a "serial" execution.  
By not considering all elements, the output skyline set generated by each core will be a **superset** of the final skyline.  
The result for the single input element is expressed by a boolean value (whether it is part of the skyline or not).  
At an implementation level, it is expressed as an integer value (0 or 1), but conceptually there is no difference.  
The final skyline is the result of the **intersection** of all the output sets from each core.  
It is obtained through a **reduction** that uses the mathematical **multiplication operator** between all the local sets of integers (in this case, both conceptually and practically, it is like performing an intersection between sets or an AND operation between multiple boolean values).

## 4.2. Analysis of Collected Data
The **onerous communication costs** in distributed memory technology mean that **linear speedup never occurs**.  
This is also consistent with the increasing number of cores, as the amount of memory that needs to be moved also increases with them (each core produces an output of $N$ elements, one more core means $N$ more elements to share).
# 5. Conclusion
## 5.1. Strong Scalability
From the collected data represented in the graphs, it is evident that both technologies behave consistently with the definition of **strong scalability**.  
The scalability ratio, as the number of cores increases, tends to equate the ratio $\frac{\text{Work of processor } p}{\text{Additional technology costs}}$, and this is also a symptom absolutely in line with the concept of strong scalability.